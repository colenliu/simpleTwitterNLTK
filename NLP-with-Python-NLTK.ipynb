{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRO**\n",
    "\n",
    "- unstructured text data extremely common, analyzing large quantities of text data is key way to understand what people are thinking help find trending news topics\n",
    "- tweets on twitter, reviews on amazon helps users purchase best-rated products\n",
    "These examples of organizing and structuring knowledge represent **Natural Language Processing (NLP) tasks**\n",
    "\n",
    "NLP: field of cpsc that focuses on interaction b/w cpus and humans. NLP techniques are used to analyze text, providing a way for cpus to understand human language. A few examples of NLP applications:\n",
    "- automatic summarization\n",
    "- topic segmentation\n",
    "- sentiment analysis\n",
    "\n",
    "this tutorial uses **Natural Language Toolkit (NLTK)**: an NLP tool for Python\n",
    "\n",
    "**PREREQS**\n",
    "- have python 3 installed\n",
    "- local programming environment setup\n",
    "\n",
    "**STEP 1: IMPORT NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK import statement (doesn't really work)\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2: DOWNLOAD NLTK'S DATA AND TAGGER**\n",
    "\n",
    "We will use a twitter corpus that we can download thru NLTK. Specifically, work w/ NLTK's \"twitter_samples\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\liuco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real import statement\n",
    "import nltk\n",
    "\n",
    "# download twitter_samples from nltk.corpus\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's twitter corpus contains sample of 20k tweets retrieved from **twitter streaming API**. Full tweets are stored as line-separated **JSON**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how many JSON files exist in corpus\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use file IDs to return tweet strings\n",
    "\n",
    "#test: twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know our **corpus** was downloaded successfully. We have access to twitter_samples corpus, so now we can begin writing a script to process tweets.\n",
    "\n",
    "The goal of our script will be to count how many adjectives and nouns appear in the positive subset of the twitter_samples corpus: **nouns and adjectives**\n",
    "\n",
    "Later, we could extend script to count positive adjectives (great, happy, etc) vs. negative adjectives (boring, lame, sad, etc.) which could be used to **analyze sentiment** of tweets/reviews of a product/movie. For example, this script provides data that can in turn inform decisions related to that product or movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3: TOKENIZING SENTENCES**\n",
    "1. create sript that we'll be working in + call it nlp.py\n",
    "2. let's import corpus\n",
    "3. create tweets variable\n",
    "4. assign it to list of tweet strings from positive_tweets.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\liuco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import statement\n",
    "import nltk\n",
    "\n",
    "# download twitter_samples\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "#set tweets to list of tweet strings from positive_tweets.json file\n",
    "tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When first load tweets, each tweet = 1 string. Before we can determine which words in tweets are adjs/nouns, need to **tokenize** tweets.\n",
    "\n",
    "**TOKENIZATION**: act of breaking up sequence of strings into pieces such as words, keywords, phrases, symbols, and other elements, which are called **tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new variable: tweets_tokens\n",
    "tweets_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweets_tokens: is a new variable that is a list where each element in list is a list of tokens. Now that we have tokens of each tweet, we can tag tokens w/ appropriate POS tags\n",
    "\n",
    "**STEP 4: TAGGING SENTENCES**\n",
    "To access NLTK's POS tagger, need to import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\liuco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import statement \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# tag each of our tokens by creating new variable, tweets_tagged, which we will use to store tagged lists\n",
    "tweets_tagged = pos_tag_sents(tweets_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our tweet is represented as a **list** and for each token, we have info about its POS tag. Each token/tag pair is saved as a **tuple**.\n",
    "\n",
    "in NLTK taggers:\n",
    "- abbreviation for adjective is **JJ**\n",
    "- abbreviation for singular nouns is **NN**\n",
    "- abbreviation for pluaral nouns is **NNS**\n",
    "\n",
    "In next step, we will count how many times **JJ** and **NN** appear throughout corpus\n",
    "\n",
    "**STEP 5: COUNTING POS TAGS**\n",
    "\n",
    "- note: POS = part of sentence\n",
    "\n",
    "We will keep track of how many times **JJ** and **NN** appear using **accumulator (count) variable** which we will continuously add to every time we find a tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, crate a count that is set to 0 initially\n",
    "JJ_count = 0\n",
    "NN_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create variables, create 2 for loops. First loop iterates thru each tweet in list. Second loop iterates thru each token/tag pair in each tweet. For each pair, will look up tag using appropriate tuple index.\n",
    "\n",
    "Next, check to see if tag matches either string **'JJ'** or **'NN'*8 using **conditional statements**. If tag is a match, add +=1 to appropriate accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st for loop: iterate thru each tweet in list\n",
    "for tweet in tweets_tagged:\n",
    "    # 2nd for loop: iterates thru each token/tag pair in each tweet\n",
    "    for pair in tweet:\n",
    "        # for each pair, look up tag \n",
    "        tag = pair[1]\n",
    "        if tag == 'JJ':\n",
    "            JJ_count += 1\n",
    "        elif tag == 'NN':\n",
    "            NN_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2 loops complete, should have total count for adjectives and nouns in our corpus, add print statements to see how many adjectives/nouns found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # adjectives =  6094\n",
      "Total # nouns =  13180\n"
     ]
    }
   ],
   "source": [
    "print('Total # adjectives = ', JJ_count)\n",
    "print('Total # nouns = ', NN_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**user-friendly view of all code written**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of adjectives =  6094\n",
      "Total number of nouns =  13180\n"
     ]
    }
   ],
   "source": [
    "# Import data and tagger\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag_sents\n",
    "\n",
    "# Load tokenized tweets\n",
    "tweets_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "\n",
    "# Tag tagged tweets\n",
    "tweets_tagged = pos_tag_sents(tweets_tokens)\n",
    "\n",
    "# Set accumulators\n",
    "JJ_count = 0\n",
    "NN_count = 0\n",
    "\n",
    "# Loop through list of tweets\n",
    "for tweet in tweets_tagged:\n",
    "    for pair in tweet:\n",
    "        tag = pair[1]\n",
    "        if tag == 'JJ':\n",
    "            JJ_count += 1\n",
    "        elif tag == 'NN':\n",
    "            NN_count += 1\n",
    "\n",
    "# Print total numbers for each adjectives and nouns\n",
    "print('Total number of adjectives = ', JJ_count)\n",
    "print('Total number of nouns = ', NN_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**\n",
    "\n",
    "- learned some NLP techniques to analyze test using NLTK library in python\n",
    "- able to download corpora, tokenize, tag, and count POS tags in python\n",
    "- maybe try now w/ real twitter data or reading in own data (plain text files?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
